{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "from utils import load_data, val_loglhood, loglhood, jump_dst, reconstruct_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling(iters, data_path, K, p, q, mh_iters=1, init_mle=False, n_rows=None, debug=False,\\\n",
    "                   method='normal', X=None, Y0=None, annealing=False, T=None, annealing_n=None,\\\n",
    "                   date_start=None, date_end=None, plant_names=None):\n",
    "    \"\"\"\n",
    "    iters: quantity of samples of A and U.\n",
    "    data_path: path where data is saved.\n",
    "    K: number of plants (n_plants in load_data function).\n",
    "    p: past time to be considered.\n",
    "    q: jumping distribution for parameters (from scipy.stats).\n",
    "    mh_iters: haw many samples do with Metropolis Hastings.\n",
    "    n_rows: how many rows of the data to consider.\n",
    "    debug: debug mode.\n",
    "    method: normal - use a jumping distribution from scipy.stats\n",
    "            personalized - use the jumping distribution personalized by us.\n",
    "    X, Y0: data to use directly without using load_data function.\n",
    "    annealing: boolean, use simulated annealing in MH.\n",
    "    T: simulated annealing decay function.\n",
    "    \"\"\"\n",
    "    if X is None or Y0 is None:\n",
    "        print('Loading data...')\n",
    "        Y0, X = load_data(data_path, K, p, resample_rule='10T', n_rows=n_rows,\\\n",
    "                          date_start=date_start, date_end=date_end, plant_names=plant_names)\n",
    "    print('Y0 shape: {}'.format(Y0.shape))\n",
    "    print('X shape: {}'.format(X.shape))\n",
    "\n",
    "    # Theta is the vector of all parameters that will be sampled.\n",
    "    # A and CovU are reshaped to a 1-D vector theta.\n",
    "    # Note that this theta change dimensionality when using personalized.\n",
    "    print('Initializing parameters...')\n",
    "    theta = init_parameters(K, p, q, Y0, X, debug=debug, method=method)\n",
    "    \n",
    "    \n",
    "    if init_mle:\n",
    "        print('Calculating MLE...')\n",
    "        f = lambda theta: -val_loglhood(theta,Y0,X,False, method=method, init_params=False)\n",
    "        result = minimize(f, theta)\n",
    "        theta = result.x\n",
    "        print('Init MLE theta calculated! ({})'.format(-result.fun))\n",
    "    \n",
    "    if p == 1 and method == 'personalized':\n",
    "        A, CovU = reconstruct_coefs(theta, K)\n",
    "    else:\n",
    "        A    = np.reshape(theta[:p*K**2],(K*p,K)).swapaxes(0,1)\n",
    "        CovU = np.reshape(theta[p*K**2:],(K,K)).swapaxes(0,1)\n",
    "        CovU = np.dot(CovU.T,CovU)\n",
    "    print(A)\n",
    "    print(CovU)\n",
    "    \n",
    "    if debug:\n",
    "        print('Parameters intialized!')\n",
    "    samples = []\n",
    "    for i in range(iters):\n",
    "        start_it = time.time()\n",
    "        print('Iteration {}'.format(i))\n",
    "\n",
    "        # Loop over all parameters and for each parameter theta[j],\n",
    "        # do a MH sampling over the distribution of theta[j] given theta[-j].\n",
    "        for j in range(theta.shape[0]):\n",
    "            start = time.time()\n",
    "            mh_samples = metropolis_hastings(theta, j, q, mh_iters, Y0, X, K, debug,\\\n",
    "                                             method=method, annealing=annealing, T=T,\\\n",
    "                                             annealing_n=annealing_n)\n",
    "            end = time.time()\n",
    "            # print('Time for sampling theta[{}]: {}'.format(j, end - start))\n",
    "            # When mh_iters > 1, mh_samples contain mh_iters samples, so a random\n",
    "            # choice (uniform) is done for selection of the new theta.\n",
    "            theta[j] = np.random.choice(mh_samples)\n",
    "        \n",
    "        lk = val_loglhood(theta,Y0,X,False, method=method, init_params=False)\n",
    "        print('LK of new theta: {}'.format(lk))\n",
    "\n",
    "        if p == 1 and method == 'personalized':\n",
    "            A, CovU = reconstruct_coefs(theta, K)\n",
    "        else:\n",
    "            A    = np.reshape(theta[:p*K**2],(K*p,K)).swapaxes(0,1)\n",
    "            CovU = np.reshape(theta[p*K**2:],(K,K)).swapaxes(0,1)\n",
    "        samples.append([A.copy(), CovU.copy()])\n",
    "        end_it = time.time()\n",
    "        print('Time for iteration {0}: {1:.2f} segs.'.format(i, end_it - start_it))\n",
    "        remaining_time = ((end_it - start_it) * (iters - (i + 1))) / 60\n",
    "        print('Estimated remaining time: {0:.2f} mins.'.format(remaining_time))\n",
    "    print('Finished!')\n",
    "    return samples\n",
    "\n",
    "\n",
    "def metropolis_hastings(theta, j, q, iters, Y0, X, K, debug, method='normal',\\\n",
    "                        annealing=False, T=None, annealing_n=None):\n",
    "    \"\"\"\n",
    "    theta: theta vector with all parameters.\n",
    "    j: theta index of the parameter currently been sampled.\n",
    "    q: jumping distribution.\n",
    "    \"\"\"\n",
    "    user_std = 1\n",
    "    samples_mh = [theta[j]] # start sample.\n",
    "    lk_old = val_loglhood(theta, Y0, X, debug, method=method)\n",
    "    # print('init lk: {}'.format(lk_old))\n",
    "    accepted = 0\n",
    "    rejected = 0\n",
    "    for t in range(iters):\n",
    "        lk_new = -np.inf\n",
    "        c = -1\n",
    "        while lk_new == -np.inf:\n",
    "            c += 1\n",
    "            if method == 'normal':\n",
    "                x_new = q.rvs(loc=samples_mh[-1], scale=1)\n",
    "                theta[j] = x_new\n",
    "            elif method == 'personalized':\n",
    "                theta, q_eval_new, q_eval_old = jump_dst(theta, j, user_std, K)\n",
    "            lk_new = val_loglhood(theta, Y0, X, debug, method=method)\n",
    "            # print('new_lk: {}'.format(lk_new))\n",
    "        #print('Quantity of -np.infs: {}'.format(c))\n",
    "        if method == 'normal':\n",
    "            if annealing and t <= annealing_n:\n",
    "                logalpha = min([(T(t) ** -1) * (lk_new - lk_old + np.log(q.pdf(samples_mh[-1], loc=x_new) \\\n",
    "                                / q.pdf(x_new, loc=samples_mh[-1]))), 0])\n",
    "            else:\n",
    "                logalpha = min([lk_new - lk_old + np.log(q.pdf(samples_mh[-1], loc=x_new) \\\n",
    "                                / q.pdf(x_new, loc=samples_mh[-1])), 0])\n",
    "        elif method == 'personalized':\n",
    "            if annealing and t <= annealing_n:\n",
    "                logalpha = min([(T(t) ** -1) * (lk_new - lk_old + np.log(q_eval_old / q_eval_new)), 0])\n",
    "            else:\n",
    "                logalpha = min([lk_new - lk_old + np.log(q_eval_old / q_eval_new), 0])\n",
    "        alpha = np.exp(logalpha)\n",
    "        u = stats.uniform.rvs()\n",
    "        if u < alpha:\n",
    "            #print('acepted')\n",
    "            samples_mh.append(theta[j])\n",
    "            lk_old = lk_new\n",
    "            accepted += 1\n",
    "        else:\n",
    "            #print('rejected')\n",
    "            rejected += 1\n",
    "            samples_mh.append(samples_mh[-1])\n",
    "            theta[j] = samples_mh[-1]\n",
    "\n",
    "    #print('accepted: {}%%'.format(accepted * 100 / (accepted + rejected)))\n",
    "    #print(samples_mh)\n",
    "    return np.array(samples_mh)\n",
    "        \n",
    "    \n",
    "def init_parameters(K, p, q, Y0, X, method='normal', debug=False):\n",
    "    \"\"\"\n",
    "    Initialization of parameters. This functions search a matrix A\n",
    "    and a matrix CovU that satisfy some conditions that A and CovU\n",
    "    must satisfy.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print('Initializing parameters...')\n",
    "    while True:\n",
    "        theta = np.zeros(K ** 2 * (p + 1))\n",
    "        for i in range(theta.shape[0]):\n",
    "            theta[i] = q.rvs()\n",
    "\n",
    "        # Force CovU to be positive semidefinite.\n",
    "        covu = np.reshape(theta[-K**2:], (K, K)).T\n",
    "        covu = np.dot(covu.T, covu)\n",
    "        theta[-K**2:] = np.reshape(covu, K**2)\n",
    "        \n",
    "        lk = val_loglhood(theta, Y0, X, debug, method=method, init_params=True)\n",
    "        if debug:\n",
    "            print('LK = {}'.format(lk))\n",
    "        if lk != -np.inf:\n",
    "            print('lk init: {}'.format(lk))\n",
    "            if p == 1 and method == 'personalized':\n",
    "                A = np.reshape(theta[:p*K**2],(K*p,K)).swapaxes(0,1)\n",
    "                eig_valuesA, eig_vecA = np.linalg.eig(A)\n",
    "                eig_valuesB, eig_vecB = np.linalg.eig(covu)\n",
    "                theta = np.concatenate((eig_vecA.reshape(-1), eig_vecB.reshape(-1),\n",
    "                                        eig_valuesA, eig_valuesB))\n",
    "                if np.all(np.isreal(eig_valuesA)):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/chrisams/Documents/datasets/data_TAIM/processed/'\n",
    "date_start = '2011-05'\n",
    "date_end = '2011-06'\n",
    "plant_names = [\n",
    "    'd05b_2010-06-19_2018-03-05.csv',\n",
    "    'd01_2009-07-12_2018-01-17.csv',\n",
    "]\n",
    "\n",
    "q = stats.norm\n",
    "K = 2\n",
    "p = 1\n",
    "iters = 2\n",
    "debug = False\n",
    "mh_iters = 10\n",
    "n_rows = None # Number of rows of the data to load\n",
    "method = 'normal'\n",
    "init_mle = False\n",
    "annealing = True\n",
    "T0 = 300\n",
    "TF = 1\n",
    "annealing_n = 5\n",
    "X=None\n",
    "Y0=None\n",
    "T = lambda t: T0 * ((TF / T0) ** (t / annealing_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Reading d05b_2010-06-19_2018-03-05.csv...\n",
      "Reading d01_2009-07-12_2018-01-17.csv...\n",
      "Y0 shape: (2, 8783)\n",
      "X shape: (2, 8783)\n",
      "Initializing parameters...\n",
      "lk init: -80524.5338115304\n",
      "[[ 0.01567817 -0.54348559]\n",
      " [ 0.54656043 -0.43252028]]\n",
      "[[ 0.15441977 -0.04256723]\n",
      " [-0.04256723  3.14359328]]\n",
      "Iteration 0\n",
      "LK of new theta: 2165.284179475292\n",
      "Time for iteration 0: 38.16\n",
      "Estimated remaining time: 0.64 mins.\n",
      "Iteration 1\n",
      "LK of new theta: 6064.435168425403\n",
      "Time for iteration 1: 43.31\n",
      "Estimated remaining time: 0.00 mins.\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "samples = gibbs_sampling(iters, DATA_PATH, K, p, q, mh_iters=mh_iters, init_mle=init_mle, n_rows=n_rows,\\\n",
    "                         debug=False, method='normal', X=X, Y0=Y0, annealing=annealing, T=T,\\\n",
    "                         annealing_n=annealing_n, date_start=date_start, date_end=date_end,\\\n",
    "                         plant_names=plant_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiempo normal: 108.39511632919312\n",
    "\n",
    "tiempo personalizado :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./samples.pickle', 'wb') as f:\n",
    "    pickle.dump(samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-0.65870902,  0.10967603],\n",
       "         [-0.59949607, -0.39017218]]), array([[1.39691405, 1.50945839],\n",
       "         [0.47145804, 1.28749427]])], [array([[-0.65870902,  0.10967603],\n",
       "         [-0.64259651, -0.07038095]]), array([[1.52771601, 1.50945839],\n",
       "         [0.47145804, 1.28749427]])]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./samples.pickle', 'rb') as f:\n",
    "    samples_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-0.65870902,  0.10967603],\n",
       "         [-0.59949607, -0.39017218]]), array([[1.39691405, 1.50945839],\n",
       "         [0.47145804, 1.28749427]])], [array([[-0.65870902,  0.10967603],\n",
       "         [-0.64259651, -0.07038095]]), array([[1.52771601, 1.50945839],\n",
       "         [0.47145804, 1.28749427]])]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/chrisams/Documents/datasets/data_TAIM/processed/'\n",
    "K = 3\n",
    "theta_old = np.ones(K*K*2+K*2)\n",
    "j = 0\n",
    "user_std = 1\n",
    "n_rows = 10000\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0, X = load_data(DATA_PATH, K, 1, resample_rule='10T', n_rows=n_rows)\n",
    "theta_new, q_eval_new, q_eval_old = jump_dst(theta_old, j, user_std, K)\n",
    "lk_new = val_loglhood(theta_new, Y0, X, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y0.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.pdf(1, loc=2, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.pdf(1, loc=2, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "U = np.array([[7, 8, 9], [10, 11, 12], [13, 14, 15]])\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Av = A.T.reshape(-1)\n",
    "print(Av)\n",
    "Uv = U.T.reshape(-1)\n",
    "print(Uv)\n",
    "theta = np.concatenate([Av, Uv])\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kv = 3\n",
    "pv = 1\n",
    "A = np.reshape(theta[:pv*Kv**2],(Kv*pv,Kv)).swapaxes(0,1)\n",
    "CovU = np.reshape(theta[pv*Kv**2:],(Kv,Kv)).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)\n",
    "print(CovU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "a = np.array([[1, 2, 3], [2, 1, 4], [3, 4, 5]])\n",
    "b = np.array([[1, 2, 16], [2, 1, 4], [16, 4, 5]])\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_valuesA, eig_vecA = np.linalg.eig(a)\n",
    "eig_valuesB, eig_vecB = np.linalg.eig(b)\n",
    "theta = np.concatenate((eig_vecA.reshape(-1),eig_vecB.reshape(-1),\n",
    "                        eig_valuesA,eig_valuesB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_vecA = np.reshape(theta[:(K*K)],(K,K))\n",
    "samp_vecU = np.reshape(theta[(K*K):(K*K*2)],(K,K))\n",
    "samp_valA = np.diag(theta[(K*K*2):(K*K*2+K)])\n",
    "samp_valU = np.diag(theta[(K*K*2+K):(K*K*2+K*2)])\n",
    "\n",
    "A = samp_vecA @ samp_valA @np.linalg.inv(samp_vecA)\n",
    "U = samp_vecU @ samp_valU @np.linalg.inv(samp_vecU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
